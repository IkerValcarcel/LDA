{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#from tqdm import tqdm.notebook.tqdm as tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240d779",
   "metadata": {},
   "source": [
    "## Se importan los dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all metadata\n",
    "#hau nire diskan daukat SAD karpetan 2020-2021\n",
    "\n",
    "video_games = []#\n",
    "insurances = []   \n",
    "#dfMergedfMeta= pd.read_csv('Cell_Phones/SamsungAppleXiaomiReviews_prueba.csv')\n",
    "video_games= pd.read_csv('VG-Reviews5AndMetaElecNintSonyMic_v2.csv')\n",
    "insurances = pd.read_csv('HRBlockIntuitReviewsTrainDev_vLast7.csv', low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c616d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_games.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021b42d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "video_games['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5327286",
   "metadata": {},
   "outputs": [],
   "source": [
    "insurances.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "insurances['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d878d",
   "metadata": {},
   "source": [
    "## Se obtienen en dataframes aparte las distintas marcas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNintendo = video_games[video_games['brand'].str.contains('Nintendo',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995cf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSony = video_games[video_games['brand'].str.contains('Sony|PlayStation|Electr',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMicrosoft = video_games[video_games['brand'].str.contains('[M|m]icrosoft',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96115ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIntuit = insurances[insurances['brand'].str.contains('Intuit')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c304ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHR = insurances[~insurances['brand'].str.contains('Intuit')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e0f62",
   "metadata": {},
   "source": [
    "## Se separan las reviews negativas y positivas de cada dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa65796",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNintendoPositives=dfNintendo[dfNintendo['overall']>3]\n",
    "dfNintendoNegatives=dfNintendo[dfNintendo['overall']<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61624c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSonyPositives=dfSony[dfSony['overall']>3]\n",
    "dfSonyNegatives=dfSony[dfSony['overall']<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e062310",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMicrosoftPositives=dfMicrosoft[dfMicrosoft['overall']>3]\n",
    "dfMicrosoftNegatives=dfMicrosoft[dfMicrosoft['overall']<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIntuitPositives = dfIntuit[dfIntuit['overall']>3]\n",
    "dfIntuitNegatives = dfIntuit[dfIntuit['overall']<=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684535e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHRPositives = dfHR[dfHR['overall']>3]\n",
    "dfHRNegatives = dfHR[dfHR['overall']<=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7158fc",
   "metadata": {},
   "source": [
    "## Se obtienen las text reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendoPositivesDocuments = dfNintendoPositives[dfNintendoPositives['reviewText'].notna()]['reviewText'].tolist()\n",
    "nintendoNegativesDocuments = dfNintendoNegatives[dfNintendoNegatives['reviewText'].notna()]['reviewText'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973ae4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonyPositivesDocuments = dfSonyPositives[dfSonyPositives['reviewText'].notna()]['reviewText'].tolist()\n",
    "sonyNegativesDocuments = dfSonyNegatives[dfSonyNegatives['reviewText'].notna()]['reviewText'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a66c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "microsoftPositivesDocuments = dfMicrosoftPositives[dfMicrosoftPositives['reviewText'].notna()]['reviewText'].tolist()\n",
    "microsoftNegativesDocuments = dfMicrosoftNegatives[dfMicrosoftNegatives['reviewText'].notna()]['reviewText'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc85b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intuitPositivesDocuments = dfIntuitPositives[dfIntuitPositives['reviewText'].notna()]['reviewText'].tolist()\n",
    "intuitNegativesDocuments = dfIntuitNegatives[dfIntuitNegatives['reviewText'].notna()]['reviewText'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HRPositivesDocuments = dfHRPositives[dfHRPositives['reviewText'].notna()]['reviewText'].tolist()\n",
    "HRNegativesDocuments = dfHRNegatives[dfHRNegatives['reviewText'].notna()]['reviewText'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94968efd",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c11086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(H, W, feature_names, documents,output_name, no_top_words = 30, no_top_documents = 10):\n",
    "    f = open(output_name + \".txt\", \"a\")\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        s = \"Topic: \" + str(topic_idx) + \" -->\"\n",
    "        print(s)\n",
    "        f.write(s)\n",
    "        s = ''.join([' ' +feature_names[i] + ' ' + str(round(topic[i], 5)) \n",
    "                for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        print(s)\n",
    "        f.write(s)\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        docProbArray=np.argsort(W[:,topic_idx])\n",
    "        print(docProbArray)\n",
    "    \n",
    "        #for doc_index in top_doc_indices:\n",
    "        #    print(documents[doc_index])\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb600785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa770c",
   "metadata": {},
   "source": [
    "### LDAHRPos_11_0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 11\n",
    "\n",
    "alpha = 0.8\n",
    "beta = 0.8\n",
    "\n",
    "documents = HRPositivesDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f555c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=no_topics, doc_topic_prior=alpha, topic_word_prior=beta, max_iter=100, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfcae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_H, lda_W, tf_feature_names, documents, 'LDAHRPos_11_0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc82192",
   "metadata": {},
   "source": [
    "### LDAHRNeg_6_0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 6\n",
    "\n",
    "alpha = 0.8\n",
    "beta = 0.8\n",
    "\n",
    "documents = HRNegativesDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a971eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=no_topics, doc_topic_prior=alpha, topic_word_prior=beta, max_iter=100, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5110519",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_H, lda_W, tf_feature_names, documents, 'LDAHRNeg_6_0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61a212",
   "metadata": {},
   "source": [
    "### LDAIntuitPos_16_0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 16\n",
    "\n",
    "alpha = 0.8\n",
    "beta = 0.8\n",
    "\n",
    "documents = intuitPositivesDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bc8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63187330",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=no_topics, doc_topic_prior=alpha, topic_word_prior=beta, max_iter=100, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(lda_H, lda_W, tf_feature_names, documents, 'LDAIntuitPos_16_0.8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aafde3",
   "metadata": {},
   "source": [
    "## GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61563167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1050c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproceso(docs):\n",
    "    # Tokenize \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "    # Bag-of-words\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "    return corpus , dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model(num_topics,corpus, dictionary):\n",
    "    chunksize = 2000\n",
    "    passes = 20\n",
    "    iterations = 400\n",
    "    eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    # Make an index to word dictionary.\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ad1bb",
   "metadata": {},
   "source": [
    "### GensimMicroPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631732a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,dictionary = preproceso(sonyPositivesDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('GensimSonyPos[10-100]' + \".txt\", \"a\")\n",
    "for i in range(10,101):\n",
    "    print(\"Generando modelo: \" + str(i))\n",
    "    model=gen_model(i,corpus)\n",
    "    top_topics = model.top_topics(corpus)\n",
    "    f.write(top_topics)\n",
    "    f.write(\"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78326a0",
   "metadata": {},
   "source": [
    "### GensimMicroNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,dictionary = preproceso(sonyNegativesDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('GensimSonyNeg[10-100]' + \".txt\", \"a\")\n",
    "for i in range(10,101):\n",
    "    print(\"Generando modelo: \" + str(i))\n",
    "    model_act=gen_model(i,corpus)\n",
    "    top_topics = model_act.top_topics(corpus)\n",
    "    f.write(top_topics)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
